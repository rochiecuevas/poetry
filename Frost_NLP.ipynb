{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poets, meet Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies to read the SQLite database\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from time import time\n",
    "from IPython.core.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(poet):\n",
    "    \"\"\" Load the data from database into a dataframe \"\"\"\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {poet};\", conn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the poetry database\n",
    "conn = sqlite3.connect(\"db/Poetry.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique poets\n",
    "poet_list = [\"Frost\", \"Yeats\", \"Kipling\"]\n",
    "\n",
    "# Iterate through the list to create a list of dataframes\n",
    "poems_df = [create_dataframe(poet) for poet in poet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disconnect from the poetry database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case there are nulls in the dataframe, convert these to NaN\n",
    "for x in range(len(poems_df)):\n",
    "    poems_df[x].replace(\"None\", np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year of publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about publication year (various sources)\n",
    "pubyear_Frost = [1913, 1916, 1928, 1914, 1916, 1916, 1923, 1923, 1928, 1923,\n",
    "           1923, 1923, 1920, 1914, 1923, 1913, 1914, 1913, 1917, 1923,\n",
    "           1916, 1913, 1923, 1923, 1923, 1914, 1942, 1923, 1916, 1914,\n",
    "           1916, 1918, 1916, 1923, 1913, 1914, 1920]\n",
    "\n",
    "pubyear_Yeats = [1938, 1914, 1933, 1916, 1921, 1919, 1904, 1913, 1919, 1933,\n",
    "                 1932, 1889, 1916, 1898, 1927, 1938, 1904, 1916, 1921, 1915,\n",
    "                 1938, 1909, 1928, 1916, 1916, 1899, 1939, 1916, 1899, 1916,\n",
    "                 1899, 1917, 1892, 1914, 1917, 1889, 1921, 1889, 1899, 1892,\n",
    "                 1928, 1917, 1914, 1889, 1892, 1892, 1892, 1933, 1914, 1933,\n",
    "                 1917, 1914, 1933, 1912, 1919, 1935, 1917, 1914, 1934, 1934,\n",
    "                 1934, 1916, 1916, 1935, 1916, 1919, 1912, 1919, 1914, 1916,\n",
    "                 1916, 1912, 1919, 1916, 1916, 1914, 1912, 1934, 1914, 1912,\n",
    "                 1914, 1916]\n",
    "\n",
    "pubyear_Kipling = [1922] * 416 + [1919, 1922, 1920, 1902, 1904, 1895, 1904, 1917, 1895, 1916, \n",
    "                                  1920, 1919, 1922, 1921, 1922, 1919, 1902, 1922, 1904, 1895,\n",
    "                                  1917, 1920, 1895, 1922, 1896, 1895, 1922, 1895, 1917, 1917,\n",
    "                                  1920, 1915, 1922, 1922]\n",
    "\n",
    "pubyears_list = [pubyear_Frost, pubyear_Yeats, pubyear_Kipling] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(poems_df)):\n",
    "    poems_df[x][\"pub_year\"] = pubyears_list[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>lines</th>\n",
       "      <th>poet</th>\n",
       "      <th>pub_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Prelude</td>\n",
       "      <td>https://www.bartleby.com/364/1.html</td>\n",
       "      <td>(To Departmental Ditties)I HAVE eaten your bre...</td>\n",
       "      <td>Rudyard Kipling</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A General Summary</td>\n",
       "      <td>https://www.bartleby.com/364/2.html</td>\n",
       "      <td>WE are very slightly changed\\nFrom the semi-ap...</td>\n",
       "      <td>Rudyard Kipling</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Army Headquarters</td>\n",
       "      <td>https://www.bartleby.com/364/3.html</td>\n",
       "      <td>Old is the song that I sing\\nOld as my unpaid ...</td>\n",
       "      <td>Rudyard Kipling</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Study of an Elevation, in Indian Ink</td>\n",
       "      <td>https://www.bartleby.com/364/4.html</td>\n",
       "      <td>This ditty is a string of lies.\\nButhow the de...</td>\n",
       "      <td>Rudyard Kipling</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Delilah</td>\n",
       "      <td>https://www.bartleby.com/364/5.html</td>\n",
       "      <td>We have another Viceroy now, those days are de...</td>\n",
       "      <td>Rudyard Kipling</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                 title  \\\n",
       "0      0                               Prelude   \n",
       "1      1                     A General Summary   \n",
       "2      2                     Army Headquarters   \n",
       "3      3  Study of an Elevation, in Indian Ink   \n",
       "4      4                               Delilah   \n",
       "\n",
       "                                  link  \\\n",
       "0  https://www.bartleby.com/364/1.html   \n",
       "1  https://www.bartleby.com/364/2.html   \n",
       "2  https://www.bartleby.com/364/3.html   \n",
       "3  https://www.bartleby.com/364/4.html   \n",
       "4  https://www.bartleby.com/364/5.html   \n",
       "\n",
       "                                               lines             poet  \\\n",
       "0  (To Departmental Ditties)I HAVE eaten your bre...  Rudyard Kipling   \n",
       "1  WE are very slightly changed\\nFrom the semi-ap...  Rudyard Kipling   \n",
       "2  Old is the song that I sing\\nOld as my unpaid ...  Rudyard Kipling   \n",
       "3  This ditty is a string of lies.\\nButhow the de...  Rudyard Kipling   \n",
       "4  We have another Viceroy now, those days are de...  Rudyard Kipling   \n",
       "\n",
       "   pub_year  \n",
       "0      1922  \n",
       "1      1922  \n",
       "2      1922  \n",
       "3      1922  \n",
       "4      1922  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the dataframes\n",
    "poems_df[2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that selects the relevant columns\n",
    "def BuildDataframe(df):\n",
    "    \"\"\" Create a new dataframe containing metadata \"\"\"\n",
    "    \n",
    "    df1 = df[[\"title\", \"poet\", \"pub_year\"]] # Select the relevant columns from the dataframe\n",
    "    \n",
    "    lines = df[\"lines\"].values.tolist() # Convert the lines column into a list of strings\n",
    "    \n",
    "    processed = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if pd.isnull(line):\n",
    "            processed.append(line) # Keep the \"nan\" value\n",
    "        else:\n",
    "            x = line.replace(\"\\n\", \" \") # Replace special characters with spaces\n",
    "            x = x.lower() # Convert the string to lower case\n",
    "            processed.append(x) # Add to empty list\n",
    "    \n",
    "    df1[\"lines\"] = processed\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 ms, sys: 4.78 ms, total: 113 ms\n",
      "Wall time: 115 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/PythonData/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# For each dataframe, choose the relevant columns\n",
    "%time poems_df1 = [BuildDataframe(df) for df in poems_df] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>poet</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>October</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>1913</td>\n",
       "      <td>o hushed october morning mild, thy leaves have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘Out, Out—’</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>1916</td>\n",
       "      <td>the buzz saw snarled and rattled in the yard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acquainted with the Night</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>1928</td>\n",
       "      <td>i have been one acquainted with the night. i h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After Apple-Picking</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>1914</td>\n",
       "      <td>my long two-pointed ladder's sticking through ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Birches</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>1916</td>\n",
       "      <td>when i see birches bend to left and right acro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title          poet  pub_year  \\\n",
       "0                    October  Robert Frost      1913   \n",
       "1                ‘Out, Out—’  Robert Frost      1916   \n",
       "2  Acquainted with the Night  Robert Frost      1928   \n",
       "3        After Apple-Picking  Robert Frost      1914   \n",
       "4                    Birches  Robert Frost      1916   \n",
       "\n",
       "                                               lines  \n",
       "0  o hushed october morning mild, thy leaves have...  \n",
       "1  the buzz saw snarled and rattled in the yard a...  \n",
       "2  i have been one acquainted with the night. i h...  \n",
       "3  my long two-pointed ladder's sticking through ...  \n",
       "4  when i see birches bend to left and right acro...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview each dataframe\n",
    "# [0] = Frost, [1] = Yeats, [2] = Kipling\n",
    "poems_df1[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the dataframes\n",
    "poems_df2 = pd.concat(poems_df1)\n",
    "poems_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(539, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with null values\n",
    "poems_df3 = poems_df2.dropna()\n",
    "\n",
    "# Drop rows with duplicate titles\n",
    "poems_df3 = poems_df3.drop_duplicates(subset = \"title\", keep = \"first\")\n",
    "\n",
    "# Drop rows with duplicate titles (after inspecting the dataframe)\n",
    "poems_df3 = poems_df3.drop(poems_df3.index[[149, 534, 538]])\n",
    "\n",
    "# Reset dataframe index\n",
    "poems_df3 = poems_df3.reset_index(drop = True)\n",
    "\n",
    "# Inspect shape of the resulting dataframe\n",
    "poems_df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poem length and lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of titles, lines, and poets \n",
    "titles_list = poems_df3[\"title\"].values.tolist()\n",
    "lines_list = poems_df3[\"lines\"].values.tolist()\n",
    "poets_list = poems_df3[\"poet\"].values.tolist()\n",
    "years_list = poems_df3[\"pub_year\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get length of the entire poem for each poem in each dataframe\n",
    "lengths_list = [len(lines.split()) for lines in lines_list]\n",
    "len(lengths_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import re, string\n",
    "\n",
    "import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenise, Remove Stop Words, Lemmatise\n",
    "Reference for lemmatisation: https://marcobonzanini.com/2015/01/26/stemming-lemmatisation-and-pos-tagging-with-python-and-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from the list\n",
    "stops = stopwords.words(\"english\")\n",
    "exclude = list(set(string.punctuation)) + ['’', '‘']\n",
    "stops.extend([\"shall\", \"isnt\", \"youve\", \"youll\", \"youre\", \"dont\", \"cant\", \"hath\",\n",
    "              \"thee\", \"thou\", \"weve\"])\n",
    "\n",
    "# Lemmatise the words in each list to retain their roots\n",
    "lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(text, conversion_dict, before = None):\n",
    "    if not text:\n",
    "        return text\n",
    "    else:\n",
    "        before = before or str.lower\n",
    "        t = before(text)\n",
    "        for key, value in conversion_dict.items():\n",
    "            t = t.replace(key, value)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniser(poem):\n",
    "    \"\"\" Processes the poem into tokens and removes stop words, numbers, and punctuations \"\"\"\n",
    "    \n",
    "    word_dict = {\"mistresssilver\": \"mistress silver\", \"winesit\": \"wine sit\",\n",
    "                 \"ironcold\": \"iron cold\", \"ironcan\": \"iron can\", \"ironto\": \"ironto\",\n",
    "                 \"valiantsceptres\": \"valiant sceptres\", \"ironis\": \"iron is\",\n",
    "                 \"ironmust\": \"iron must\", \"treasoni\": \"treason i\",\n",
    "                 \"ironshall\": \"iron shall\", \n",
    "                 \"ironwas\": \"iron was\"}\n",
    "    \n",
    "    pattern = \"|\".join(sorted(re.escape(k) for k in word_dict))\n",
    "     \n",
    "    words = poem[0: poem.find(\"note .\")] # remove all substrings that start with \"note .\"\n",
    "    words1 = words.replace(\"[back]\", \"\") # remove all \"[back]\" substrings\n",
    "    words2 = words1.replace(\"—\", \" \") # remove all \"—\" substrings\n",
    "#     words3 = words2.replace(\"mistresssilver\", \"mistress silver\")\n",
    "    words3 = translator(words2, word_dict)\n",
    "    words4 = re.sub(\"[^a-zA-Z0-9 ]\",\" \", words3) # Replace punctuations with white space\n",
    "    words5 = re.sub(\" +\", \" \", words4).strip() # Remove extra white spaces\n",
    "    \n",
    "    words6 = word_tokenize(words5) # Create a list of words\n",
    "    words7 = [word for word in words6 if word not in stops] # Filter the keywords\n",
    "    words8 = [word for word in words7 if word not in exclude] # Filter the keywords\n",
    "    words9 = [lemmatiser.lemmatize(word, pos = \"v\") for word in words8] # Lemmatise each word\n",
    "    words_list = [wd for wd in words9 if len(wd) > 2]\n",
    "    \n",
    "    preprocessed_text = \" \".join(words_list) # Convert the list of strings back to one string\n",
    "\n",
    "    \n",
    "    return words_list, preprocessed_text # Returns a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 539 calculations.\n"
     ]
    }
   ],
   "source": [
    "# Tokenise each poem in each dataframe and create a processed text\n",
    "start_time = ()\n",
    "tokenised_poems = []   \n",
    "for x in range(len(lines_list)):\n",
    "    print(f\"Now calculating for {x}th poem.\")\n",
    "    %time y = tokeniser(lines_list[x]) \n",
    "    tokenised_poems.append(y)\n",
    "    clear_output(wait = True)\n",
    "print(f\"End of {len(lines_list)} calculations.\")    \n",
    "\n",
    "# Unpack the tuples into lists of tokens and of filtered poems\n",
    "df_tokens, df_filtered = map(list, zip(*tokenised_poems))\n",
    "\n",
    "# Determine the length of each filtered poem\n",
    "df_filtered_length = [len(df) for df in df_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add three new columns to each dataframe for tokens and filtered text\n",
    "poems_df3[\"tokens\"] = df_tokens\n",
    "poems_df3[\"filteredPoem\"] = df_filtered\n",
    "poems_df3[\"filteredLength\"] = df_filtered_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>poet</th>\n",
       "      <th>pub_year</th>\n",
       "      <th>lines</th>\n",
       "      <th>tokens</th>\n",
       "      <th>filteredPoem</th>\n",
       "      <th>filteredLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>October</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>1913</td>\n",
       "      <td>o hushed october morning mild, thy leaves have...</td>\n",
       "      <td>[hush, october, morning, mild, thy, leave, rip...</td>\n",
       "      <td>hush october morning mild thy leave ripen fall...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘Out, Out—’</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>1916</td>\n",
       "      <td>the buzz saw snarled and rattled in the yard a...</td>\n",
       "      <td>[buzz, saw, snarl, rattle, yard, make, dust, d...</td>\n",
       "      <td>buzz saw snarl rattle yard make dust drop stav...</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acquainted with the Night</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>1928</td>\n",
       "      <td>i have been one acquainted with the night. i h...</td>\n",
       "      <td>[one, acquaint, night, walk, rain, back, rain,...</td>\n",
       "      <td>one acquaint night walk rain back rain outwalk...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After Apple-Picking</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>1914</td>\n",
       "      <td>my long two-pointed ladder's sticking through ...</td>\n",
       "      <td>[long, two, point, ladder, stick, tree, toward...</td>\n",
       "      <td>long two point ladder stick tree toward heaven...</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Birches</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>1916</td>\n",
       "      <td>when i see birches bend to left and right acro...</td>\n",
       "      <td>[see, birch, bend, leave, right, across, line,...</td>\n",
       "      <td>see birch bend leave right across line straigh...</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title          poet  pub_year  \\\n",
       "0                    October  Robert Frost      1913   \n",
       "1                ‘Out, Out—’  Robert Frost      1916   \n",
       "2  Acquainted with the Night  Robert Frost      1928   \n",
       "3        After Apple-Picking  Robert Frost      1914   \n",
       "4                    Birches  Robert Frost      1916   \n",
       "\n",
       "                                               lines  \\\n",
       "0  o hushed october morning mild, thy leaves have...   \n",
       "1  the buzz saw snarled and rattled in the yard a...   \n",
       "2  i have been one acquainted with the night. i h...   \n",
       "3  my long two-pointed ladder's sticking through ...   \n",
       "4  when i see birches bend to left and right acro...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [hush, october, morning, mild, thy, leave, rip...   \n",
       "1  [buzz, saw, snarl, rattle, yard, make, dust, d...   \n",
       "2  [one, acquaint, night, walk, rain, back, rain,...   \n",
       "3  [long, two, point, ladder, stick, tree, toward...   \n",
       "4  [see, birch, bend, leave, right, across, line,...   \n",
       "\n",
       "                                        filteredPoem  filteredLength  \n",
       "0  hush october morning mild thy leave ripen fall...              77  \n",
       "1  buzz saw snarl rattle yard make dust drop stav...             148  \n",
       "2  one acquaint night walk rain back rain outwalk...              55  \n",
       "3  long two point ladder stick tree toward heaven...             139  \n",
       "4  see birch bend leave right across line straigh...             239  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the dataframe\n",
    "poems_df3 = poems_df3.reset_index(drop = True)\n",
    "poems_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 81 poems written by William Butler Yeats.\n",
      "There are 36 poems written by Robert Frost.\n",
      "There are 422 poems written by Rudyard Kipling.\n"
     ]
    }
   ],
   "source": [
    "# Number of poems\n",
    "poets = list(set(poets_list))\n",
    "\n",
    "for x in range(len(poets)):\n",
    "    print (f\"There are {poems_df3.loc[poems_df3['poet'] == poets[x]].shape[0]} poems written by {poets[x]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: William Butler Yeats\n",
      "\n",
      "Longest Poem: Title = The Two Kings, Length = 1010\n",
      "Shortest Poem: Title = A Needle's Eye, Length = 12\n",
      "Average Poem Length = 110.0\n",
      "=====\n",
      "\n",
      "Author: Robert Frost\n",
      "\n",
      "Longest Poem: Title = The Death of the Hired Man, Length = 619\n",
      "Shortest Poem: Title = Dust of Snow, Length = 15\n",
      "Average Poem Length = 120.0\n",
      "=====\n",
      "\n",
      "Author: Rudyard Kipling\n",
      "\n",
      "Longest Poem: Title = MAndrews Hymn, Length = 1189\n",
      "Shortest Poem: Title = There was a small boy of Quebec, Length = 13\n",
      "Average Poem Length = 187.0\n",
      "=====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Longest and shorted poems and their lengths\n",
    "for x in range(len(poets)):\n",
    "\n",
    "    df = poems_df3[poems_df3[\"poet\"] == poets[x]]\n",
    "    longest_length = df[\"filteredLength\"].max()\n",
    "    shortest_length = df[\"filteredLength\"].min()\n",
    "    mean_length = round(df[\"filteredLength\"].mean(),0)\n",
    "    \n",
    "    longest_title = df[df[\"filteredLength\"] == longest_length][\"title\"].item().encode(\"ascii\", \"ignore\")\\\n",
    "    .decode(\"utf-8\")\n",
    "    shortest_title = df[df[\"filteredLength\"] == shortest_length][\"title\"].item().encode(\"ascii\", \"ignore\")\\\n",
    "    .decode(\"utf-8\")\n",
    "        # .encode(\"ascii\", \"ignore\").decode(\"utf-8\") removes the ascii and the unicode characters\n",
    "        # .item() extracts the value of each element in a pandas series\n",
    "    \n",
    "    print(f\"Author: {poets[x]}\\n\")\n",
    "    print(f\"Longest Poem: Title = {longest_title}, Length = {longest_length}\")\n",
    "    print(f\"Shortest Poem: Title = {shortest_title}, Length = {shortest_length}\")\n",
    "    print(f\"Average Poem Length = {mean_length}\")\n",
    "    print(\"=====\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique words; how many unique words per poem?\n",
    "unique_words = [list(set(x)) for x in df_tokens]\n",
    "length_unique_words = [len(x) for x in unique_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical diversity: proportion of unique words among all the words in the poem\n",
    "# I used the filtered words\n",
    "lex_div = [round(length_unique_words[i] / df_filtered_length[i],3) for i in range(len(length_unique_words))]\n",
    "lex_div[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word importance\n",
    "Source: https://stevenloria.com/tf-idf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import math\n",
    "from textblob import TextBlob as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that calculates term frequency\n",
    "def tf(word, poem):\n",
    "    return poem.words.count(word) / len(poem.words)\n",
    "\n",
    "# Create a function that determines the number of documents that contain a certain word\n",
    "def n_docs(word, poemlist):\n",
    "    return sum(1 for poem in poemlist if word in poem.words)\n",
    "\n",
    "# Create a function that determines the inverse document frequency (IDF)\n",
    "# IDF = how common a word is among all the documents in poemlist\n",
    "def idf(word, poemlist):\n",
    "    return math.log(len(poemlist) / (1 + n_docs(word, poemlist)))\n",
    "\n",
    "def tdidf(word, poem, poemlist):\n",
    "    return tf(word, poem) * idf(word, poemlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the poemlist from df[\"lines\"]\n",
    "%time poemlist = [tb(poem) for poem in poems_df3[\"filteredPoem\"]]\n",
    "poemlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to be filled with text blobs from cleaning poemlist\n",
    "poemlist2 = []\n",
    "\n",
    "# Loop through the poemlist\n",
    "for i in range(0, len(poemlist)):\n",
    "    \n",
    "    # Remove words that are shorter than 3 characters\n",
    "    new_string = ' '.join([w for w in str(poemlist[i]).split() if len(w) > 3])\n",
    "    \n",
    "    # Replace emm dash with space\n",
    "    new_string2 = new_string.replace(\"—\", \" \")\n",
    "    \n",
    "    # Convert string to text blob\n",
    "    new_string2 = tb(new_string2)\n",
    "    \n",
    "    # Append the text blob to the list of text blobs\n",
    "    poemlist2.append(new_string2)\n",
    "    \n",
    "poemlist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the most important words\n",
    "start_time = time()\n",
    "impt_words = []\n",
    "for i, poem in enumerate(poemlist2):\n",
    "    print(f\"Now processing {i}th poem.\")\n",
    "    scores = {word: tdidf(word, poem, poemlist2) for word in poem.words}\n",
    "    sorted_words = sorted(scores.items(), key = lambda x: x[1], reverse = True)\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    for word, score in sorted_words[:5]:\n",
    "        impt_words.append((i, word, round(score, 5)))\n",
    "        \n",
    "elapsed_time = time() - start_time\n",
    "print(f\"End of {len(poemlist2)} calculations. Elapsed time: {round(elapsed_time/60, 3)} min.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impt_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of indices (article numbers) and poets\n",
    "poet_dict = dict(zip(poems_df3.index, poems_df3.poet.tolist()))\n",
    "title_dict = dict(zip(poems_df3.index, poems_df3.title.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of important words per poem\n",
    "poems_df4 = pd.DataFrame(impt_words, columns = [\"PoemNo\", \"Word\", \"TF-IDF\"])\n",
    "\n",
    "# Map the poets and the titles to the important words based on the dictionary\n",
    "poems_df4[\"Poet\"] = poems_df4[\"PoemNo\"].map(poet_dict)\n",
    "poems_df4[\"Title\"] = poems_df4[\"PoemNo\"].map(title_dict)\n",
    "poems_df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tf-idf as a sqlite database table (for Javascript use later)\n",
    "conn = sqlite3.connect(\"db/Poetry.db\")\n",
    "\n",
    "# Create a database table from the dataframe\n",
    "poems_df4.to_sql(\"tfidf\", conn, if_exists = \"replace\", index = False)\n",
    "\n",
    "# Preview the database table\n",
    "pd.read_sql_query(\"select * from tfidf;\", conn).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the important words by poem title\n",
    "poems_df5 = pd.DataFrame(poems_df4.groupby([\"Title\", \"Word\"])[\"TF-IDF\"].mean())\n",
    "poems_df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\")\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import widgets, interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a widget containing poem titles (sorted alphabetically)\n",
    "titles = titles_list\n",
    "titles.sort()\n",
    "\n",
    "poem_title = widgets.Dropdown(options = [\"Choose a poem...\"] + titles, value = \"Choose a poem...\", \n",
    "                              description = \"Title:\", disabled = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filter based on title\n",
    "def plot_it(poem_title):\n",
    "    if poem_title != \"Choose a poem...\":\n",
    "        poems_df6 = poems_df4[poems_df4[\"Title\"] == poem_title]\n",
    "        \n",
    "        plt.figure(figsize = (10, 6))\n",
    "        sns.set(font_scale = 1.5)\n",
    "        graph = sns.barplot(y = \"Word\", x = \"TF-IDF\", data = poems_df6, palette = \"Blues_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the data by poem title\n",
    "interactive(plot_it, poem_title = poem_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis - Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiments based on textblobs\n",
    "sentiment_polarity = [round(poem.sentiment.polarity, 3) \\\n",
    "                      for poem in poemlist2]\n",
    "sentiment_cat = [\"positive\" if sp > 0\n",
    "                 else \"negative\" if sp < 0\n",
    "                 else \"neutral\"\n",
    "                 for sp in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.DataFrame({\"PoemNo\": list(poet_dict.keys()),\n",
    "                         \"Poet\": list(poet_dict.values()),\n",
    "                         \"Title\": list(title_dict.values()),\n",
    "                         \"Content\": poems_df3[\"lines\"].tolist(),\n",
    "                         \"Length\": df_filtered_length,\n",
    "                         \"Sentiment\": sentiment_cat,\n",
    "                         \"Pubn_Year\": poems_df3[\"pub_year\"].tolist(),\n",
    "                         \"Lexical_Diversity\": lex_div}, \n",
    "                        columns = [\"PoemNo\", \"Poet\", \"Title\", \"Length\", \n",
    "                                   \"Content\", \"Sentiment\", \"Pubn_Year\", \"Lexical_Diversity\"])\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata as a sqlite database table (for Javascript use later)\n",
    "conn = sqlite3.connect(\"db/Poetry.db\")\n",
    "\n",
    "# Create a database table from the dataframe\n",
    "metadata.to_sql(\"metadata\", conn, if_exists = \"replace\", index = False)\n",
    "\n",
    "# Preview the database table\n",
    "pd.read_sql_query(\"select * from metadata;\", conn).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling\n",
    "Sources: \n",
    "1. https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "2. https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim \n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and the trigram model\n",
    "tokens = df_tokens\n",
    "bigram = gensim.models.Phrases(tokens, min_count = 5, threshold = 100)\n",
    "trigram = gensim.models.Phrases(bigram[tokens], threshold = 100) \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words = stops, allowed_postags = [\"NOUN\", \"ADJ\", \"ADV\"]):\n",
    "    \"\"\" Remove stop words, create bigrams and trigrams, lemmatise \"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]    \n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    texts_out = []\n",
    "    \n",
    "    nlp = spacy.load(\"en\", disable = [\"parser\", \"ner\"])\n",
    "    \n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        \n",
    "        # remove stop words (again)         \n",
    "        texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] \\\n",
    "                     for doc in texts_out]\n",
    "        \n",
    "        # remove words shorter than three letters       \n",
    "        texts_out = [[word for word in lst if len(word) > 2] for lst in texts_out]\n",
    "\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter text (again!)\n",
    "%time filtered_text = process_words(tokens)\n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "%time id2words = corpora.Dictionary(filtered_text)\n",
    "\n",
    "# Create corpus term frequency (convert dictionary to bag-of-words)\n",
    "%time corpus = [id2words.doc2bow(text) for text in filtered_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of number of topics\n",
    "num_topics = list(range(1, 21))\n",
    "num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates the coherence score \n",
    "def coherence_score(num_topics):\n",
    "    \"\"\" Create a LDA model \"\"\"\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                            id2word = id2words,\n",
    "                                            num_topics = num_topics,\n",
    "                                            random_state = 100,\n",
    "                                            update_every = 1,\n",
    "                                            chunksize = 600,\n",
    "                                            passes = 100,\n",
    "                                            alpha = \"auto\",\n",
    "                                            per_word_topics = True)\n",
    "    \n",
    "    \"\"\" Calculate the coherence score \"\"\"\n",
    "    coherence_model_lda = CoherenceModel(model = lda_model, \n",
    "                                         texts = filtered_text, \n",
    "                                         dictionary = id2words,\n",
    "                                         coherence = 'c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    \n",
    "    \"\"\" Perplexity \"\"\"\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    \n",
    "    return coherence_lda, lda_model, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the coherence score of each number of topics\n",
    "start_time = time()\n",
    "\n",
    "coh_score = []\n",
    "for x in num_topics:\n",
    "    print(f\"Now processing {x} topic/s.\")\n",
    "    y = coherence_score(x)[0]\n",
    "    elapsed_time = time() - start_time\n",
    "    print(f\"Elapsed time: {round(elapsed_time/60, 3)} min.\")\n",
    "    \n",
    "    coh_score.append(y)\n",
    "    \n",
    "    print(f\"Calculation for {x} topic completed.\")  \n",
    "    clear_output(wait = True)\n",
    "print(f\"End of {len(num_topics)} calculations.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of topics vs coherence score\n",
    "# Find the highest coherence score before the trend flattens out\n",
    "plt.plot(num_topics, coh_score, \"bo-\")\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Coherence score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the LDA model using the chosen number of topics\n",
    "final_number = 3\n",
    "%time coh_score2 = coherence_score(final_number)\n",
    "\n",
    "# Compute Perplexity\n",
    "print(f\"Perplexity: {coh_score2[2]}\")\n",
    "print(f\"Coherence Score: {coh_score2[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords for the top 10 topics\n",
    "%time doc_lda = coh_score2[1][corpus]\n",
    "pprint(coh_score2[1].print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graphs of most important words per topic\n",
    "# Based on the LDA model\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "%time panel = pyLDAvis.gensim.prepare(coh_score2[1], corpus, id2words)\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph as a html page\n",
    "pyLDAvis.save_html(panel, \"lda.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominant Topic in each poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(doc_lda, ldamodel = coh_score2[1], texts = tokens):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(doc_lda):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list \n",
    "        row = sorted(row, key = lambda x: (x[1]), reverse = True)\n",
    "        \n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), \\\n",
    "                                                                  topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(doc_lda)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of text per tokenised poem\n",
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "plt.figure(figsize = (10,3), dpi = 160)\n",
    "plt.hist(doc_lens, bins = 1000, color='navy')\n",
    "plt.text(200, 10, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(200, 8.5, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(200, 7, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(200, 5.5, \"1%ile    : \" + str(round(np.quantile(doc_lens, q = 0.01))))\n",
    "plt.text(200, 4, \"99%ile  : \" + str(round(np.quantile(doc_lens, q = 0.99))))\n",
    "\n",
    "plt.gca().set(xlim = (0, 300), ylabel = 'Number of Documents', xlabel = 'Document Word Count')\n",
    "plt.tick_params(size = 8)\n",
    "plt.xticks(np.linspace(0, 300, 9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict = dict(size = 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(3, figsize = (16,14), dpi = 160, sharex = True, sharey = True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):    \n",
    "    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
    "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
    "    ax.hist(doc_lens, bins = 1000, color = cols[i])\n",
    "    ax.tick_params(axis = 'y', labelcolor = cols[i], color = cols[i])\n",
    "    sns.kdeplot(doc_lens, color = \"black\", shade = False, ax = ax.twinx())\n",
    "    ax.set(xlim = (0, 300), xlabel = 'Document Word Count')\n",
    "    ax.set_ylabel('Number of Documents', color = cols[i])\n",
    "    ax.set_title('Topic: '+str(i), fontdict = dict(size = 8, color = cols[i]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top = 0.90)\n",
    "plt.xticks(np.linspace(0, 300, 9))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud of Top N words in each topic\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords = stops,\n",
    "                  background_color = 'white',\n",
    "                  width = 2500,\n",
    "                  height = 1800,\n",
    "                  max_words = 10,\n",
    "                  colormap = 'tab10',\n",
    "                  color_func = lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal = 1.0)\n",
    "\n",
    "topics = coh_score2[1].show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, figsize = (10,10), sharex = True, sharey = True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size = 300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size = 16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "plt.axis('off')\n",
    "plt.margins(x = 0, y = 0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = coh_score2[1].show_topics(formatted = False)\n",
    "data_flat = [w for w_list in filtered_text for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df2 = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])   \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(3, figsize = (16,10), sharey = True, dpi = 160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x = 'word', \n",
    "           height = \"word_count\", \n",
    "           data = df2.loc[df2.topic_id == i, :], \n",
    "           color = cols[i], \n",
    "           width = 0.5, \n",
    "           alpha = 0.3, \n",
    "           label = 'Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x = 'word', \n",
    "                height = \"importance\", \n",
    "                data = df2.loc[df2.topic_id == i, :], \n",
    "                color = cols[i], width = 0.2,\n",
    "                label ='Weights')\n",
    "    ax.set_ylabel('Word Count', color = cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 50)\n",
    "    ax.set_title('Topic: ' + str(i), color = cols[i], fontsize = 16)\n",
    "    ax.tick_params(axis = 'y', left = False)\n",
    "    ax.set_xticklabels(df2.loc[df2.topic_id == i, 'word'], \n",
    "                       rotation = 30, \n",
    "                       horizontalalignment = 'right')\n",
    "    ax.legend(loc ='upper left')\n",
    "    ax_twin.legend(loc = 'upper right')\n",
    "\n",
    "fig.tight_layout(w_pad = 2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize = 14, y = 1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word colouring of N poems\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model = coh_score2[1], corpus = corpus, start = 0, end = 38):\n",
    "    corp = corpus[start:end]\n",
    "    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
    "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment = 'center',\n",
    "                    fontsize = 16, color = 'black', transform = ax.transAxes, fontweight = 700)\n",
    "\n",
    "            # Draw Rectangle\n",
    "            topic_percs_sorted = sorted(topic_percs, key = lambda x: (x[1]), reverse = True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill = None, alpha = 1, \n",
    "                                   color = mycolors[topic_percs_sorted[0][0]], linewidth = 2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.5, word,\n",
    "                            horizontalalignment = 'left',\n",
    "                            verticalalignment = 'center',\n",
    "                            fontsize = 16, \n",
    "                            color = mycolors[topics],\n",
    "                            transform = ax.transAxes, \n",
    "                            fontweight = 700)\n",
    "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment = 'left',\n",
    "                    verticalalignment = 'center',\n",
    "                    fontsize = 16, \n",
    "                    color = 'black',\n",
    "                    transform = ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "    plt.suptitle('Topic Coloring for Poems: ' + str(start) + ' to ' + str(end-2), \\\n",
    "                 fontsize = 14, y = 0.95, fontweight = 700)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sentences_chart() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that identifies and quantifies the dominant topics\n",
    "def topics_per_document(model, corpus, start = 0, end = 1):\n",
    "    corpus_sel = corpus[start:end]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    for i, corp in enumerate(corpus_sel):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse = True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "    \n",
    "    return(dominant_topics, topic_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topics, topic_percentages = topics_per_document(model = coh_score2[1], corpus = corpus, end = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Dominant Topics in Each Document\n",
    "df = pd.DataFrame(dominant_topics, columns = ['Document_Id', 'Dominant_Topic'])\n",
    "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name = 'count').reset_index()\n",
    "\n",
    "# Total Topic Distribution by actual weight\n",
    "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name = 'count').reset_index()\n",
    "\n",
    "# Top 3 Keywords for each Topic\n",
    "topic_top3words = [(i, topic) for i, topics in coh_score2[1].show_topics(formatted = False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns = ['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level =0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize = (10, 4), dpi = 120, sharex = True)\n",
    "\n",
    "# Topic Distribution by Dominant Topics\n",
    "ax1.bar(x = 'Dominant_Topic', \n",
    "        height = 'count', \n",
    "        data = df_dominant_topic_in_each_doc, \n",
    "        width = .5, color = 'firebrick')\n",
    "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + \\\n",
    "                               df_top3words.loc[df_top3words.topic_id == x, 'words'].values[0])\n",
    "ax1.xaxis.set_major_formatter(tick_formatter)\n",
    "ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size = 10))\n",
    "ax1.set_ylabel('Number of Documents')\n",
    "ax1.set_ylim(0, 10)\n",
    "\n",
    "# Topic Distribution by Topic Weights\n",
    "ax2.bar(x = 'index', \n",
    "        height = 'count', \n",
    "        data = df_topic_weightage_by_doc, \n",
    "        width = .5, color = 'steelblue')\n",
    "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
    "ax2.xaxis.set_major_formatter(tick_formatter)\n",
    "ax2.set_title('Number of Documents by Topic Weightage', fontdict = dict(size = 10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a keyword network based on term frequency and TF-IDF\n",
    "(use the \"to_gephi.csv\" and \"to_gephi2.csv\" files in Gephi for visualisation)\n",
    "source: https://pythondata.com/text-analytics-visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that extracts the most common words per poem\n",
    "def get_keywords(token_list, num):\n",
    "    return Counter(token_list).most_common(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the filtered poems into strings\n",
    "poemlist3 = [str(poem) for poem in poemlist2]\n",
    "token_list = [word_tokenize(poem) for poem in poemlist3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({\"title\": list(title_dict.values()), \n",
    "                    \"poet\": list(poet_dict.values()),\n",
    "                    \"filteredPoem\": poemlist3})\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to extract the top 5 words per poem\n",
    "keywords = [get_keywords(tokens, 5) for tokens in token_list]\n",
    "\n",
    "# Extract the list of keywords \n",
    "unzipped = [zip(*kw)for kw in keywords]\n",
    "kw = [list(x)[0] for x in unzipped]\n",
    "\n",
    "# Convert the list of keywords to a string\n",
    "kw2 = [\",\".join(str(y) for y in x) for x in kw]\n",
    "\n",
    "# Add the list of keywords to the dataframe\n",
    "df4[\"keywords_TF\"] = kw2\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add keywords based on TF-IDF\n",
    "impt_words2 = poems_df5.reset_index().groupby(\"Title\")[\"Word\"].apply(list)\n",
    "df4[\"keywords_TF-IDF\"] = [\",\".join(str(y) for y in x) for x in impt_words2]\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of keywords according to term frequency\n",
    "keywordsTF = []\n",
    "for i, r in df4.iterrows():\n",
    "    keywords = r[\"keywords_TF\"].split(\",\")\n",
    "    for kw in keywords:\n",
    "        keywordsTF.append((kw.strip(\"\"), r[\"keywords_TF\"]))\n",
    "kwTF_df = pd.DataFrame(keywordsTF).rename(columns = {0: \"keyword\", 1: \"keywords\"})\n",
    "kwTF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of keywords according to TF-IDF\n",
    "keywordsTFIDF = []\n",
    "for i, r in df4.iterrows():\n",
    "    keywords = r[\"keywords_TF-IDF\"].split(\",\")\n",
    "    for kw in keywords:\n",
    "        keywordsTFIDF.append((kw.strip(\"\"), r[\"keywords_TF-IDF\"]))\n",
    "kwTFIDF_df = pd.DataFrame(keywordsTFIDF).rename(columns = {0: \"keyword\", 1: \"keywords\"})\n",
    "kwTFIDF_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rows to lists\n",
    "docsTF = kwTF_df[\"keywords\"].tolist()\n",
    "namesTF = kwTF_df[\"keyword\"].tolist()\n",
    "\n",
    "docs_list = [i.split(\",\")for i in docsTF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an ordered dictionary of keyword and frequency of co-occurrence\n",
    "from collections import OrderedDict\n",
    "occurrences = OrderedDict((name, OrderedDict((name, 0) for name in namesTF)) for name in namesTF)\n",
    "\n",
    "for i in docs_list:\n",
    "    for x in range(len(i)):\n",
    "        for item in i[:x] + i[x + 1:]:\n",
    "            occurrences[i[x]][item] += 1\n",
    "\n",
    "# Create a dataframe of co-occurrences\n",
    "co_occur_df = pd.DataFrame.from_dict(occurrences)         \n",
    "co_occur_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occur_df.to_csv(\"to_gephi.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rows to lists\n",
    "docsTFIDF = kwTFIDF_df[\"keywords\"].tolist()\n",
    "namesTFIDF = kwTFIDF_df[\"keyword\"].tolist()\n",
    "\n",
    "docs_list = [i.split(\",\")for i in docsTFIDF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ordered dictionary of keyword and frequency of co-occurrence\n",
    "from collections import OrderedDict\n",
    "occurrences2 = OrderedDict((name, OrderedDict((name, 0) for name in namesTFIDF)) for name in namesTFIDF)\n",
    "\n",
    "for i in docs_list:\n",
    "    for x in range(len(i)):\n",
    "        for item in i[:x] + i[x + 1:]:\n",
    "            occurrences2[i[x]][item] += 1\n",
    "\n",
    "# Create a dataframe of co-occurrences\n",
    "co_occur_df2 = pd.DataFrame.from_dict(occurrences2)         \n",
    "co_occur_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occur_df2.to_csv(\"to_gephi2.csv\", sep = \",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
