{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poets, meet Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies to read the SQLite database\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(poet):\n",
    "    \"\"\" Load the data from database into a dataframe \"\"\"\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {poet};\", conn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the poetry database\n",
    "conn = sqlite3.connect(\"db/Poetry.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique poets\n",
    "poet_list = [\"Frost\", \"Yeats\"]\n",
    "\n",
    "# Iterate through the list to create a list of dataframes\n",
    "poems_df = [create_dataframe(poet) for poet in poet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>lines</th>\n",
       "      <th>poet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Under Ben Bulben</td>\n",
       "      <td>https://www.poetryfoundation.org/poems/43298/u...</td>\n",
       "      <td>I\\n Swear by what the Sages spoke,\\n Round the...</td>\n",
       "      <td>William Butler Yeats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A Coat</td>\n",
       "      <td>https://www.poetryfoundation.org/poetrymagazin...</td>\n",
       "      <td>I made my song a coat,Covered with embroiderie...</td>\n",
       "      <td>William Butler Yeats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A Dialogue of Self and Soul</td>\n",
       "      <td>https://www.poetryfoundation.org/poems/43294/a...</td>\n",
       "      <td>IMy Soul. I summon to the winding ancient stai...</td>\n",
       "      <td>William Butler Yeats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A Drinking Song</td>\n",
       "      <td>https://www.poetryfoundation.org/poems/50337/a...</td>\n",
       "      <td>Wine comes in at the mouth,\\n And love comes i...</td>\n",
       "      <td>William Butler Yeats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A Meditation in Time of War</td>\n",
       "      <td>https://www.poetryfoundation.org/poems/57318/a...</td>\n",
       "      <td>For one throb of the artery, ,While on that ol...</td>\n",
       "      <td>William Butler Yeats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                        title  \\\n",
       "0      0             Under Ben Bulben   \n",
       "1      1                       A Coat   \n",
       "2      2  A Dialogue of Self and Soul   \n",
       "3      3              A Drinking Song   \n",
       "4      4  A Meditation in Time of War   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.poetryfoundation.org/poems/43298/u...   \n",
       "1  https://www.poetryfoundation.org/poetrymagazin...   \n",
       "2  https://www.poetryfoundation.org/poems/43294/a...   \n",
       "3  https://www.poetryfoundation.org/poems/50337/a...   \n",
       "4  https://www.poetryfoundation.org/poems/57318/a...   \n",
       "\n",
       "                                               lines                  poet  \n",
       "0  I\\n Swear by what the Sages spoke,\\n Round the...  William Butler Yeats  \n",
       "1  I made my song a coat,Covered with embroiderie...  William Butler Yeats  \n",
       "2  IMy Soul. I summon to the winding ancient stai...  William Butler Yeats  \n",
       "3  Wine comes in at the mouth,\\n And love comes i...  William Butler Yeats  \n",
       "4  For one throb of the artery, ,While on that ol...  William Butler Yeats  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the dataframes\n",
    "poems_df[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disconnect from the poetry database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that selects the relevant columns\n",
    "def BuildDataframe(df):\n",
    "    \"\"\" Create a new dataframe containing metadata \"\"\"\n",
    "    \n",
    "    df1 = df[[\"index\", \"title\", \"poet\"]] # Select the relevant columns from the dataframe\n",
    "    \n",
    "    lines = df[\"lines\"].values.tolist() # Convert the lines column into a list of strings\n",
    "    df1[\"lines\"] = [x.replace(\"\\n\", \" \") for x in lines] # Remove special characters and white spaces \n",
    "    df1[\"lines\"] = df1[\"lines\"].str.lower() # Put all letters in lower case\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/PythonData/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/envs/PythonData/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# For each dataframe, choose the relevant columns\n",
    "poems_df1 = [BuildDataframe(df) for df in poems_df] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>poet</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>October</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>o hushed october morning mild, thy leaves have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>‘Out, Out—’</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>the buzz saw snarled and rattled in the yard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Acquainted with the Night</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>i have been one acquainted with the night. i h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>After Apple-Picking</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>my long two-pointed ladder's sticking through ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Birches</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>when i see birches bend to left and right acro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                      title          poet  \\\n",
       "0      0                    October  Robert Frost   \n",
       "1      1                ‘Out, Out—’  Robert Frost   \n",
       "2      2  Acquainted with the Night  Robert Frost   \n",
       "3      3        After Apple-Picking  Robert Frost   \n",
       "4      4                    Birches  Robert Frost   \n",
       "\n",
       "                                               lines  \n",
       "0  o hushed october morning mild, thy leaves have...  \n",
       "1  the buzz saw snarled and rattled in the yard a...  \n",
       "2  i have been one acquainted with the night. i h...  \n",
       "3  my long two-pointed ladder's sticking through ...  \n",
       "4  when i see birches bend to left and right acro...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems_df1[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting metadata about the poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the title, lines, poet, and poem number of each poem\n",
    "poems = df1[\"title\"].values.tolist()\n",
    "index = df1[\"index\"].values.tolist()\n",
    "lines = df1[\"lines\"].values.tolist()\n",
    "poets = df1[\"poet\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about publication year (various sources)\n",
    "pubyear = [1913, 1916, 1928, 1914, 1916, 1916, 1923, 1923, 1928, 1923,\n",
    "           1923, 1923, 1920, 1914, 1923, 1913, 1914, 1913, 1917, 1923,\n",
    "           1916, 1913, 1923, 1923, 1923, 1914, 1942, 1923, 1916, 1914,\n",
    "           1916, 1918, 1916, 1923, 1913, 1914, 1920]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get length of the entire poem for each poem\n",
    "poem_length = [len(line.split()) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unique words\n",
    "unique_words = [list(set(line.split())) for line in lines]\n",
    "unique_length = [len(x) for x in unique_words]\n",
    "\n",
    "# Lexical diversity\n",
    "lex_div = [round(unique_length[i] / poem_length[i], 4) for i in range(0, len(poem_length))]\n",
    "lex_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import re, string\n",
    "\n",
    "import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenise, Remove Stop Words, Lemmatise\n",
    "Reference for lemmatisation: https://marcobonzanini.com/2015/01/26/stemming-lemmatisation-and-pos-tagging-with-python-and-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from the list\n",
    "stops = stopwords.words(\"english\")\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Lemmatise the words in each list to retain their roots\n",
    "lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the poem in preparation for word counts\n",
    "words_list = []\n",
    "preprocessed_text = []\n",
    "for poem in df1[\"lines\"]:\n",
    "    \n",
    "    # Create a list of words per poem after the words are converted to lowercase    \n",
    "    words = word_tokenize(poem)\n",
    "    \n",
    "    # Filter to remove stop words and punctuations    \n",
    "    words2 = [word for word in words if word not in stops and word not in exclude]\n",
    "    \n",
    "    # Lemmatise each word (if it's a verb, convert to root verb)\n",
    "    words3 = [lemmatiser.lemmatize(word, pos = \"v\") for word in words2]\n",
    "    \n",
    "    # Add the filtered list of words (representing each poem)\n",
    "    words_list.append(words3)\n",
    "    \n",
    "    # Convert the list of strings back to one string\n",
    "    words4 = \" \".join(words3)\n",
    "    \n",
    "    # Add the filtered list of words (representing each poem)\n",
    "    preprocessed_text.append(words4)\n",
    "\n",
    "df1[\"tokens\"] = words_list    \n",
    "df1[\"filteredPoem\"] = preprocessed_text\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that counts the number of words in each poem\n",
    "def word_count(word_list):\n",
    "    return len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the length of each filtered poem\n",
    "lengths = []\n",
    "for poem in df1[\"tokens\"]:\n",
    "    length = word_count(poem)\n",
    "    lengths.append(length)\n",
    "\n",
    "# Add the filtered poem lengths in the df\n",
    "df1[\"poemLength\"] = lengths\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of poems\n",
    "print (f\"There are {df1.shape[0]} poems written by Robert Frost in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Longest and shortest poems\n",
    "longest_poem = df1[\"poemLength\"].max()\n",
    "shortest_poem = df1[\"poemLength\"].min()\n",
    "\n",
    "for i in range(0, len(df1[\"poemLength\"])):\n",
    "    if df1[\"poemLength\"][i] == longest_poem:\n",
    "        print(f'Longest poem: {df1[\"title\"][i]}; Filtered poem length: {df1[\"poemLength\"][i]} words')\n",
    "    if df1[\"poemLength\"][i] == shortest_poem:\n",
    "        print(f'Shortest poem: {df1[\"title\"][i]}; Filtered poem length: {df1[\"poemLength\"][i]} words')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word importance\n",
    "Source: https://stevenloria.com/tf-idf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import math\n",
    "from textblob import TextBlob as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that calculates term frequency\n",
    "def tf(word, poem):\n",
    "    return poem.words.count(word) / len(poem.words)\n",
    "\n",
    "# Create a function that determines the number of documents that contain a certain word\n",
    "def n_docs(word, poemlist):\n",
    "    return sum(1 for poem in poemlist if word in poem.words)\n",
    "\n",
    "# Create a function that determines the inverse document frequency (IDF)\n",
    "# IDF = how common a word is among all the documents in poemlist\n",
    "def idf(word, poemlist):\n",
    "    return math.log(len(poemlist) / (1 + n_docs(word, poemlist)))\n",
    "\n",
    "def tdidf(word, poem, poemlist):\n",
    "    return tf(word, poem) * idf(word, poemlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the poemlist from df[\"lines\"]\n",
    "poemlist = [tb(poem) for poem in df1[\"filteredPoem\"]]\n",
    "poemlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to be filled with text blobs from cleaning poemlist\n",
    "poemlist2 = []\n",
    "\n",
    "# Loop through the poemlist\n",
    "for i in range(0, len(poemlist)):\n",
    "    \n",
    "    # Remove words that are shorter than 3 characters\n",
    "    new_string = ' '.join([w for w in str(poemlist[i]).split() if len(w) > 3])\n",
    "    \n",
    "    # Replace emm dash with space\n",
    "    new_string2 = new_string.replace(\"—\", \" \")\n",
    "    \n",
    "    # Convert string to text blob\n",
    "    new_string2 = tb(new_string2)\n",
    "    \n",
    "    # Append the text blob to the list of text blobs\n",
    "    poemlist2.append(new_string2)\n",
    "    \n",
    "poemlist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the most important words\n",
    "impt_words = []\n",
    "for i, poem in enumerate(poemlist2):\n",
    "    scores = {word: tdidf(word, poem, poemlist2) for word in poem.words}\n",
    "    sorted_words = sorted(scores.items(), key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    for word, score in sorted_words[:5]:\n",
    "        impt_words.append((i, word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impt_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of important words per poem\n",
    "df2 = pd.DataFrame(impt_words, columns = [\"PoemNo\", \"Word\", \"TF-IDF\"])\n",
    "df2[\"Poet\"] = \"Robert Frost\"\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df2 as a sqlite database table (for Javascript use later)\n",
    "conn = sqlite3.connect(\"db/Poetry.db\")\n",
    "\n",
    "# Create a database table from the dataframe\n",
    "df2.to_sql(\"tfidf\", conn, if_exists = \"replace\", index = False)\n",
    "\n",
    "# Preview the database table\n",
    "pd.read_sql_query(\"select * from tfidf;\", conn).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add titles for each poem in df2\n",
    "titles = []\n",
    "for i in range(0, len(df)):\n",
    "    for p in df2.PoemNo:\n",
    "        if i == p:\n",
    "            title = df[\"title\"][i]\n",
    "            titles.append(title) \n",
    "\n",
    "df2[\"PoemTitle\"] = titles\n",
    "\n",
    "# Preview\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the important words by poem title\n",
    "df3 = pd.DataFrame(df2.groupby([\"PoemTitle\", \"Word\"])[\"TF-IDF\"].mean())\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\")\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import widgets, interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a widget containing poem titles (sorted alphabetically)\n",
    "titles = list(df.title)\n",
    "titles.sort()\n",
    "\n",
    "poem_title = widgets.Dropdown(options = [\"Choose a poem...\"] + titles, value = \"Choose a poem...\", \n",
    "                              description = \"Title:\", disabled = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filter based on title\n",
    "def plot_it(poem_title):\n",
    "    if poem_title != \"Choose a poem...\":\n",
    "        df3 = df2[df2[\"PoemTitle\"] == poem_title]\n",
    "        \n",
    "        plt.figure(figsize = (10, 6))\n",
    "        sns.set(font_scale = 1.5)\n",
    "        graph = sns.barplot(y = \"Word\", x = \"TF-IDF\", data = df3, palette = \"Blues_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the data by poem title\n",
    "interactive(plot_it, poem_title = poem_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis - Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiments based on textblobs\n",
    "sentiment_polarity = [round(poem.sentiment.polarity, 3) \\\n",
    "                      for poem in poemlist2]\n",
    "sentiment_cat = [\"positive\" if sp > 0\n",
    "                 else \"negative\" if sp < 0\n",
    "                 else \"neutral\"\n",
    "                 for sp in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.DataFrame({\"PoemNo\": index,\n",
    "                         \"Poet\": poets,\n",
    "                         \"Title\": poems,\n",
    "                         \"Content\": lines,\n",
    "                         \"Length\": poem_length,\n",
    "                         \"Sentiment\": sentiment_cat,\n",
    "                         \"Pubn_Year\": pubyear,\n",
    "                         \"Lexical_Diversity\": lex_div}, \n",
    "                        columns = [\"PoemNo\", \"Poet\", \"Title\", \"Length\", \n",
    "                                   \"Content\", \"Sentiment\", \"Pubn_Year\", \"Lexical_Diversity\"])\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df2 as a sqlite database table (for Javascript use later)\n",
    "conn = sqlite3.connect(\"db/Poetry.db\")\n",
    "\n",
    "# Create a database table from the dataframe\n",
    "metadata.to_sql(\"metadata\", conn, if_exists = \"replace\", index = False)\n",
    "\n",
    "# Preview the database table\n",
    "pd.read_sql_query(\"select * from metadata;\", conn).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling\n",
    "Sources: \n",
    "1. https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "2. https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim \n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df1[\"tokens\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and the trigram model\n",
    "bigram = gensim.models.Phrases(tokens, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[tokens], threshold=100) \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words = stops, allowed_postags = [\"NOUN\", \"ADJ\", \"ADV\"]):\n",
    "    \"\"\" Remove stop words, create bigrams and trigrams, lemmatise \"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]    \n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    texts_out = []\n",
    "    \n",
    "    nlp = spacy.load(\"en\", disable = [\"parser\", \"ner\"])\n",
    "    \n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        \n",
    "        # remove stop words (again)         \n",
    "        texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] \\\n",
    "                     for doc in texts_out]\n",
    "        \n",
    "        # remove words shorter than three letters       \n",
    "        texts_out = [[word for word in lst if len(word) > 2] for lst in texts_out]\n",
    "\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = process_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "id2words = corpora.Dictionary(filtered_text)\n",
    "\n",
    "# Create corpus term frequency (convert dictionary to bag-of-words)\n",
    "corpus = [id2words.doc2bow(text) for text in filtered_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of number of topics\n",
    "num_topics = list(range(1, 21))\n",
    "num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates the coherence score \n",
    "def coherence_score(num_topics):\n",
    "    \"\"\" Create a LDA model \"\"\"\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                            id2word = id2words,\n",
    "                                            num_topics = num_topics,\n",
    "                                            random_state = 100,\n",
    "                                            update_every = 1,\n",
    "                                            chunksize = 100,\n",
    "                                            passes = 20,\n",
    "                                            alpha = \"auto\",\n",
    "                                            per_word_topics = True)\n",
    "    \n",
    "    \"\"\" Calculate the coherence score \"\"\"\n",
    "    coherence_model_lda = CoherenceModel(model = lda_model, \n",
    "                                         texts = filtered_text, \n",
    "                                         dictionary = id2words,\n",
    "                                         coherence = 'c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    \n",
    "    return coherence_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the coherence score of each number of topics\n",
    "coh_score = [coherence_score(x) for x in num_topics]\n",
    "coh_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of topics vs coherence score\n",
    "# Find the highest coherence score before the trend flattens out\n",
    "plt.plot(num_topics, coh_score, \"bo-\")\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Coherence score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the LDA model using the chosen number of topics\n",
    "final_number = 6\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                            id2word = id2words,\n",
    "                                            num_topics = final_number,\n",
    "                                            random_state = 100,\n",
    "                                            update_every = 1,\n",
    "                                            chunksize = 100,\n",
    "                                            passes = 20,\n",
    "                                            alpha = \"auto\",\n",
    "                                            per_word_topics = True)\n",
    "\n",
    "# Compute Perplexity\n",
    "print(f\"Perplexity: {lda_model.log_perplexity(corpus)}\")\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model = lda_model, \n",
    "                                     texts = filtered_text, \n",
    "                                     dictionary = id2words, \n",
    "                                     coherence = 'c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_lda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords for the top 10 topics\n",
    "doc_lda = lda_model[corpus]\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graphs of most important words per topic\n",
    "# Based on the LDA model\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.gensim.prepare(lda_model, corpus, id2words)\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph as a html page\n",
    "pyLDAvis.save_html(panel, \"lda.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominant Topic in each poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(doc_lda, ldamodel = lda_model, texts = tokens):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(doc_lda):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list \n",
    "        row = sorted(row, key = lambda x: (x[1]), reverse = True)\n",
    "        \n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), \\\n",
    "                                                                  topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(doc_lda, ldamodel = lda_model, texts = tokens)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of text per tokenised poem\n",
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "plt.figure(figsize = (10,3), dpi = 160)\n",
    "plt.hist(doc_lens, bins = 1000, color='navy')\n",
    "plt.text(200, 1.75, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(200, 1.60, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(200, 1.45, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(200, 1.30, \"1%ile    : \" + str(round(np.quantile(doc_lens, q = 0.01))))\n",
    "plt.text(200, 1.15, \"99%ile  : \" + str(round(np.quantile(doc_lens, q = 0.99))))\n",
    "\n",
    "plt.gca().set(xlim = (0, 300), ylabel = 'Number of Documents', xlabel = 'Document Word Count')\n",
    "plt.tick_params(size = 8)\n",
    "plt.xticks(np.linspace(0, 300, 9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict = dict(size = 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(3,2, figsize = (16,14), dpi = 160, sharex = True, sharey = True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):    \n",
    "    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
    "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
    "    ax.hist(doc_lens, bins = 1000, color = cols[i])\n",
    "    ax.tick_params(axis = 'y', labelcolor = cols[i], color = cols[i])\n",
    "    sns.kdeplot(doc_lens, color = \"black\", shade = False, ax = ax.twinx())\n",
    "    ax.set(xlim = (0, 300), xlabel = 'Document Word Count')\n",
    "    ax.set_ylabel('Number of Documents', color = cols[i])\n",
    "    ax.set_title('Topic: '+str(i), fontdict = dict(size = 8, color = cols[i]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top = 0.90)\n",
    "plt.xticks(np.linspace(0, 300, 9))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud of Top N words in each topic\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords = stops,\n",
    "                  background_color = 'white',\n",
    "                  width = 2500,\n",
    "                  height = 1800,\n",
    "                  max_words = 10,\n",
    "                  colormap = 'tab10',\n",
    "                  color_func = lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal = 1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize = (10,10), sharex = True, sharey = True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size = 300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size = 16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "plt.axis('off')\n",
    "plt.margins(x = 0, y = 0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted = False)\n",
    "data_flat = [w for w_list in filtered_text for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(3, 2, figsize = (16,10), sharey = True, dpi = 160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x = 'word', \n",
    "           height = \"word_count\", \n",
    "           data = df.loc[df.topic_id == i, :], \n",
    "           color = cols[i], \n",
    "           width = 0.5, \n",
    "           alpha = 0.3, \n",
    "           label = 'Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x = 'word', \n",
    "                height = \"importance\", \n",
    "                data = df.loc[df.topic_id == i, :], \n",
    "                color = cols[i], width = 0.2,\n",
    "                label ='Weights')\n",
    "    ax.set_ylabel('Word Count', color = cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 50)\n",
    "    ax.set_title('Topic: ' + str(i), color = cols[i], fontsize = 16)\n",
    "    ax.tick_params(axis = 'y', left = False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id == i, 'word'], \n",
    "                       rotation = 30, \n",
    "                       horizontalalignment = 'right')\n",
    "    ax.legend(loc ='upper left')\n",
    "    ax_twin.legend(loc = 'upper right')\n",
    "\n",
    "fig.tight_layout(w_pad = 2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize = 14, y = 1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word colouring of N poems\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model = lda_model, corpus = corpus, start = 0, end = 38):\n",
    "    corp = corpus[start:end]\n",
    "    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
    "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment = 'center',\n",
    "                    fontsize = 16, color = 'black', transform = ax.transAxes, fontweight = 700)\n",
    "\n",
    "            # Draw Rectangle\n",
    "            topic_percs_sorted = sorted(topic_percs, key = lambda x: (x[1]), reverse = True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill = None, alpha = 1, \n",
    "                                   color = mycolors[topic_percs_sorted[0][0]], linewidth = 2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.5, word,\n",
    "                            horizontalalignment = 'left',\n",
    "                            verticalalignment = 'center',\n",
    "                            fontsize = 16, \n",
    "                            color = mycolors[topics],\n",
    "                            transform = ax.transAxes, \n",
    "                            fontweight = 700)\n",
    "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment = 'left',\n",
    "                    verticalalignment = 'center',\n",
    "                    fontsize = 16, \n",
    "                    color = 'black',\n",
    "                    transform = ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "    plt.suptitle('Topic Coloring for Poems: ' + str(start) + ' to ' + str(end-2), \\\n",
    "                 fontsize = 14, y = 0.95, fontweight = 700)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sentences_chart() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that identifies and quantifies the dominant topics\n",
    "def topics_per_document(model, corpus, start = 0, end = 1):\n",
    "    corpus_sel = corpus[start:end]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    for i, corp in enumerate(corpus_sel):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse = True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "    \n",
    "    return(dominant_topics, topic_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topics, topic_percentages = topics_per_document(model = lda_model, corpus = corpus, end = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Dominant Topics in Each Document\n",
    "df = pd.DataFrame(dominant_topics, columns = ['Document_Id', 'Dominant_Topic'])\n",
    "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name = 'count').reset_index()\n",
    "\n",
    "# Total Topic Distribution by actual weight\n",
    "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name = 'count').reset_index()\n",
    "\n",
    "# Top 3 Keywords for each Topic\n",
    "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted = False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns = ['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level =0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (10, 4), dpi = 120, sharex = True)\n",
    "\n",
    "# Topic Distribution by Dominant Topics\n",
    "ax1.bar(x = 'Dominant_Topic', \n",
    "        height = 'count', \n",
    "        data = df_dominant_topic_in_each_doc, \n",
    "        width = .5, color = 'firebrick')\n",
    "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + \\\n",
    "                               df_top3words.loc[df_top3words.topic_id == x, 'words'].values[0])\n",
    "ax1.xaxis.set_major_formatter(tick_formatter)\n",
    "ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size = 10))\n",
    "ax1.set_ylabel('Number of Documents')\n",
    "ax1.set_ylim(0, 10)\n",
    "\n",
    "# Topic Distribution by Topic Weights\n",
    "ax2.bar(x = 'index', \n",
    "        height = 'count', \n",
    "        data = df_topic_weightage_by_doc, \n",
    "        width = .5, color = 'steelblue')\n",
    "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
    "ax2.xaxis.set_major_formatter(tick_formatter)\n",
    "ax2.set_title('Number of Documents by Topic Weightage', fontdict = dict(size = 10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a keyword network based on term frequency and TF-IDF\n",
    "(use the \"to_gephi.csv\" and \"to_gephi2.csv\" files in Gephi for visualisation)\n",
    "source: https://pythondata.com/text-analytics-visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that extracts the most common words per poem\n",
    "def get_keywords(token_list, num):\n",
    "    return Counter(token_list).most_common(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the filtered poems into strings\n",
    "poemlist3 = [str(poem) for poem in poemlist2]\n",
    "token_list = [word_tokenize(poem) for poem in poemlist3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles = df1[\"title\"].values.tolist()\n",
    "\n",
    "df4 = pd.DataFrame({\"title\": poems, \n",
    "                    \"poet\": poets,\n",
    "                    \"filteredPoem\": poemlist3})\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to extract the top 5 words per poem\n",
    "keywords = [get_keywords(tokens, 5) for tokens in token_list]\n",
    "\n",
    "# Extract the list of keywords \n",
    "unzipped = [zip(*kw)for kw in keywords]\n",
    "kw = [list(x)[0] for x in unzipped]\n",
    "\n",
    "# Convert the list of keywords to a string\n",
    "kw2 = [\",\".join(str(y) for y in x) for x in kw]\n",
    "\n",
    "# Add the list of keywords to the dataframe\n",
    "df4[\"keywords_TF\"] = kw2\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add keywords based on TF-IDF\n",
    "impt_words2 = df3.reset_index().groupby(\"PoemTitle\")[\"Word\"].apply(list)\n",
    "df4[\"keywords_TF-IDF\"] = [\",\".join(str(y) for y in x) for x in impt_words2]\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of keywords according to term frequency\n",
    "keywordsTF = []\n",
    "for i, r in df4.iterrows():\n",
    "    keywords = r[\"keywords_TF\"].split(\",\")\n",
    "    for kw in keywords:\n",
    "        keywordsTF.append((kw.strip(\"\"), r[\"keywords_TF\"]))\n",
    "kwTF_df = pd.DataFrame(keywordsTF).rename(columns = {0: \"keyword\", 1: \"keywords\"})\n",
    "kwTF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of keywords according to TF-IDF\n",
    "keywordsTFIDF = []\n",
    "for i, r in df4.iterrows():\n",
    "    keywords = r[\"keywords_TF-IDF\"].split(\",\")\n",
    "    for kw in keywords:\n",
    "        keywordsTFIDF.append((kw.strip(\"\"), r[\"keywords_TF-IDF\"]))\n",
    "kwTFIDF_df = pd.DataFrame(keywordsTFIDF).rename(columns = {0: \"keyword\", 1: \"keywords\"})\n",
    "kwTFIDF_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rows to lists\n",
    "docsTF = kwTF_df[\"keywords\"].tolist()\n",
    "namesTF = kwTF_df[\"keyword\"].tolist()\n",
    "\n",
    "docs_list = [i.split(\",\")for i in docsTF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an ordered dictionary of keyword and frequency of co-occurrence\n",
    "from collections import OrderedDict\n",
    "occurrences = OrderedDict((name, OrderedDict((name, 0) for name in namesTF)) for name in namesTF)\n",
    "\n",
    "for i in docs_list:\n",
    "    for x in range(len(i)):\n",
    "        for item in i[:x] + i[x + 1:]:\n",
    "            occurrences[i[x]][item] += 1\n",
    "\n",
    "# Create a dataframe of co-occurrences\n",
    "co_occur_df = pd.DataFrame.from_dict(occurrences)         \n",
    "co_occur_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occur_df.to_csv(\"to_gephi.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rows to lists\n",
    "docsTFIDF = kwTFIDF_df[\"keywords\"].tolist()\n",
    "namesTFIDF = kwTFIDF_df[\"keyword\"].tolist()\n",
    "\n",
    "docs_list = [i.split(\",\")for i in docsTFIDF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ordered dictionary of keyword and frequency of co-occurrence\n",
    "from collections import OrderedDict\n",
    "occurrences2 = OrderedDict((name, OrderedDict((name, 0) for name in namesTFIDF)) for name in namesTFIDF)\n",
    "\n",
    "for i in docs_list:\n",
    "    for x in range(len(i)):\n",
    "        for item in i[:x] + i[x + 1:]:\n",
    "            occurrences2[i[x]][item] += 1\n",
    "\n",
    "# Create a dataframe of co-occurrences\n",
    "co_occur_df2 = pd.DataFrame.from_dict(occurrences2)         \n",
    "co_occur_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occur_df2.to_csv(\"to_gephi2.csv\", sep = \",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
